{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Evaluation Suite: Backpack, Transformer, and Finetuned Backpack\n",
    "\n",
    "This notebook runs comprehensive evaluations for all three models:\n",
    "1. **Backpack Model** (`out/backpack_full`)\n",
    "2. **Transformer Baseline** (`out/transformer_full`)\n",
    "3. **Finetuned Backpack** (`out/backpack_finetuned`)\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- Translation BLEU Scores\n",
    "- Translation Accuracy (Exact Match, Word-level, Character-level)\n",
    "- Sentence Similarity\n",
    "- Sense Vector Analysis (Backpack models only)\n",
    "- Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Import libraries and configure device\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Add current directory to path\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from evaluate import (\n",
    "    load_model,\n",
    "    evaluate_multisimlex,\n",
    "    evaluate_cross_lingual_multisimlex,\n",
    "    analyze_sense_vectors,\n",
    "    load_test_data,\n",
    "    evaluate_translation_bleu,\n",
    "    evaluate_translation_accuracy,\n",
    "    evaluate_sentence_similarity\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Auto-detect device (use GPU if available)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Set paths to your model directories\n",
    "BASE_DIR = os.getcwd()  # Project root directory\n",
    "\n",
    "# Model directories (adjust if your paths are different)\n",
    "BACKPACK_DIR = os.path.join(BASE_DIR, 'out/backpack_full')\n",
    "TRANSFORMER_DIR = os.path.join(BASE_DIR, 'out/transformer_full')\n",
    "BACKPACK_FINETUNED_DIR = os.path.join(BASE_DIR, 'out/backpack_finetuned')\n",
    "\n",
    "# Data and evaluation settings\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data/europarl')\n",
    "LANGUAGE_PAIR = 'en-fr'\n",
    "TRANSLATION_SAMPLES = 500\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Base directory: {BASE_DIR}\")\n",
    "print(f\"  Backpack: {BACKPACK_DIR}\")\n",
    "print(f\"  Transformer: {TRANSFORMER_DIR}\")\n",
    "print(f\"  Backpack Finetuned: {BACKPACK_FINETUNED_DIR}\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Translation samples: {TRANSLATION_SAMPLES}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which models are available\n",
    "MODELS = {\n",
    "    'backpack': BACKPACK_DIR,\n",
    "    'transformer': TRANSFORMER_DIR,\n",
    "    'backpack_finetuned': BACKPACK_FINETUNED_DIR,\n",
    "}\n",
    "\n",
    "available_models = {}\n",
    "for name, path in MODELS.items():\n",
    "    ckpt_path = os.path.join(path, 'ckpt.pt')\n",
    "    if os.path.exists(ckpt_path):\n",
    "        available_models[name] = path\n",
    "        print(f\"‚úì {name}: {ckpt_path}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  {name}: {ckpt_path} (not found)\")\n",
    "\n",
    "if not available_models:\n",
    "    print(\"\\n‚ùå No models found! Please check the paths above.\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(f\"\\nüìä Found {len(available_models)} model(s): {list(available_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Full Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run full evaluation for each available model\n",
    "all_results = {}\n",
    "\n",
    "for model_name, model_dir in available_models.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EVALUATING: {model_name.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        print(f\"\\nLoading {model_name}...\")\n",
    "        model, config = load_model(model_dir, device)\n",
    "        n_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"‚úì Loaded ({n_params:,} parameters)\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        tokenizer_name = config.tokenizer_name if hasattr(config, 'tokenizer_name') else 'xlm-roberta-base'\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "        \n",
    "        results = {'model_name': model_name, 'parameters': n_params}\n",
    "        \n",
    "        # 1. Sense Vector Analysis (Backpack models only)\n",
    "        if hasattr(model, 'get_sense_vectors') or hasattr(config, 'n_senses'):\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"1. SENSE VECTOR ANALYSIS\")\n",
    "            print(f\"{'='*70}\")\n",
    "            test_words = ['hello', 'bonjour', 'world', 'monde', 'parliament', 'parlement']\n",
    "            sense_analysis = analyze_sense_vectors(model, tokenizer, test_words, device, top_k=5)\n",
    "            results['sense_analysis'] = sense_analysis\n",
    "        \n",
    "        # 2. Translation Evaluation\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"2. TRANSLATION EVALUATION\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Load test data\n",
    "        test_pairs = load_test_data(\n",
    "            data_dir=DATA_DIR,\n",
    "            language_pair=LANGUAGE_PAIR,\n",
    "            max_samples=TRANSLATION_SAMPLES,\n",
    "            split='validation'\n",
    "        )\n",
    "        \n",
    "        if test_pairs:\n",
    "            # BLEU Score\n",
    "            print(f\"\\n2a. BLEU Score Evaluation ({len(test_pairs)} pairs)...\")\n",
    "            try:\n",
    "                bleu_results = evaluate_translation_bleu(\n",
    "                    model, tokenizer, test_pairs, device,\n",
    "                    max_samples=TRANSLATION_SAMPLES,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.3,\n",
    "                    top_k=10,\n",
    "                    greedy=True\n",
    "                )\n",
    "                results['translation_bleu'] = bleu_results\n",
    "                print(f\"  ‚úì Average BLEU: {bleu_results['avg_bleu']:.4f}\")\n",
    "                print(f\"  ‚úì Median BLEU: {bleu_results['median_bleu']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error: {e}\")\n",
    "                results['translation_bleu'] = None\n",
    "            \n",
    "            # Translation Accuracy\n",
    "            print(f\"\\n2b. Translation Accuracy Evaluation...\")\n",
    "            try:\n",
    "                accuracy_results = evaluate_translation_accuracy(\n",
    "                    model, tokenizer, test_pairs, device,\n",
    "                    max_samples=TRANSLATION_SAMPLES,\n",
    "                    max_new_tokens=100,\n",
    "                    temperature=0.3,\n",
    "                    top_k=10,\n",
    "                    greedy=True\n",
    "                )\n",
    "                results['translation_accuracy'] = accuracy_results\n",
    "                print(f\"  ‚úì Exact Match: {accuracy_results['exact_match_rate']:.4f}\")\n",
    "                print(f\"  ‚úì Word Accuracy: {accuracy_results['avg_word_accuracy']:.4f}\")\n",
    "                print(f\"  ‚úì Char Accuracy: {accuracy_results['avg_char_accuracy']:.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error: {e}\")\n",
    "                results['translation_accuracy'] = None\n",
    "            \n",
    "            # Sentence Similarity\n",
    "            print(f\"\\n2c. Sentence Similarity Evaluation...\")\n",
    "            try:\n",
    "                sent_pairs = test_pairs[:min(100, len(test_pairs))]  # Use first 100 for speed\n",
    "                sent_similarities = evaluate_sentence_similarity(\n",
    "                    model, tokenizer, sent_pairs, device, method='mean'\n",
    "                )\n",
    "                if sent_similarities:\n",
    "                    similarities = [sim for _, _, sim in sent_similarities]\n",
    "                    avg_sim = sum(similarities) / len(similarities) if similarities else 0.0\n",
    "                    results['sentence_similarity'] = {\n",
    "                        'avg_similarity': avg_sim,\n",
    "                        'n_pairs': len(sent_similarities),\n",
    "                        'min_similarity': min(similarities) if similarities else 0.0,\n",
    "                        'max_similarity': max(similarities) if similarities else 0.0\n",
    "                    }\n",
    "                    print(f\"  ‚úì Average Similarity: {avg_sim:.4f}\")\n",
    "                else:\n",
    "                    results['sentence_similarity'] = None\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error: {e}\")\n",
    "                results['sentence_similarity'] = None\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  No test data loaded\")\n",
    "        \n",
    "        # Save individual results\n",
    "        all_results[model_name] = results\n",
    "        \n",
    "        # Save to file\n",
    "        output_file = os.path.join(model_dir, 'full_evaluation_results.json')\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"\\n‚úì Results saved to: {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error evaluating {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        all_results[model_name] = {'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison summary table\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EVALUATION SUMMARY - ALL MODELS\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "summary_data = []\n",
    "for model_name, results in all_results.items():\n",
    "    if 'error' in results:\n",
    "        continue\n",
    "    \n",
    "    row = {'Model': model_name}\n",
    "    \n",
    "    # BLEU scores\n",
    "    if 'translation_bleu' in results and results['translation_bleu']:\n",
    "        bleu = results['translation_bleu']\n",
    "        row['Avg BLEU'] = f\"{bleu['avg_bleu']:.4f}\"\n",
    "        row['Median BLEU'] = f\"{bleu['median_bleu']:.4f}\"\n",
    "        row['Min BLEU'] = f\"{bleu['min_bleu']:.4f}\"\n",
    "        row['Max BLEU'] = f\"{bleu['max_bleu']:.4f}\"\n",
    "    else:\n",
    "        row['Avg BLEU'] = 'N/A'\n",
    "        row['Median BLEU'] = 'N/A'\n",
    "        row['Min BLEU'] = 'N/A'\n",
    "        row['Max BLEU'] = 'N/A'\n",
    "    \n",
    "    # Accuracy scores\n",
    "    if 'translation_accuracy' in results and results['translation_accuracy']:\n",
    "        acc = results['translation_accuracy']\n",
    "        row['Exact Match'] = f\"{acc['exact_match_rate']:.4f}\"\n",
    "        row['Word Acc'] = f\"{acc['avg_word_accuracy']:.4f}\"\n",
    "        row['Char Acc'] = f\"{acc['avg_char_accuracy']:.4f}\"\n",
    "    else:\n",
    "        row['Exact Match'] = 'N/A'\n",
    "        row['Word Acc'] = 'N/A'\n",
    "        row['Char Acc'] = 'N/A'\n",
    "    \n",
    "    # Sentence similarity\n",
    "    if 'sentence_similarity' in results and results['sentence_similarity']:\n",
    "        sim = results['sentence_similarity']\n",
    "        row['Sent Sim'] = f\"{sim['avg_similarity']:.4f}\"\n",
    "    else:\n",
    "        row['Sent Sim'] = 'N/A'\n",
    "    \n",
    "    # Parameters\n",
    "    if 'parameters' in results:\n",
    "        row['Parameters'] = f\"{results['parameters']:,}\"\n",
    "    \n",
    "    summary_data.append(row)\n",
    "\n",
    "if summary_data:\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    print(\"\\n\")\n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    # Calculate improvements (Backpack vs Transformer)\n",
    "    if 'backpack' in all_results and 'transformer' in all_results:\n",
    "        bp_results = all_results['backpack']\n",
    "        tf_results = all_results['transformer']\n",
    "        \n",
    "        if ('translation_bleu' in bp_results and bp_results['translation_bleu'] and\n",
    "            'translation_bleu' in tf_results and tf_results['translation_bleu']):\n",
    "            bp_bleu = bp_results['translation_bleu']['avg_bleu']\n",
    "            tf_bleu = tf_results['translation_bleu']['avg_bleu']\n",
    "            improvement = bp_bleu - tf_bleu\n",
    "            pct_improvement = (improvement / tf_bleu * 100) if tf_bleu > 0 else 0\n",
    "            \n",
    "            print(f\"\\n{'='*70}\")\n",
    "            print(\"BACKPACK vs TRANSFORMER COMPARISON\")\n",
    "            print(f\"{'='*70}\")\n",
    "            print(f\"Average BLEU Improvement: {improvement:+.4f} ({pct_improvement:+.1f}%)\")\n",
    "            \n",
    "            bp_median = bp_results['translation_bleu']['median_bleu']\n",
    "            tf_median = tf_results['translation_bleu']['median_bleu']\n",
    "            median_improvement = bp_median - tf_median\n",
    "            median_pct = (median_improvement / tf_median * 100) if tf_median > 0 else 0\n",
    "            print(f\"Median BLEU Improvement: {median_improvement:+.4f} ({median_pct:+.1f}%)\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_file = os.path.join(BASE_DIR, 'out', 'all_models_evaluation_summary.json')\n",
    "    os.makedirs(os.path.dirname(summary_file), exist_ok=True)\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    print(f\"\\n‚úì Full results saved to: {summary_file}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No results to display\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
