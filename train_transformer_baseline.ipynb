{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79e5c97",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ecfe1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:e7c19aa6-dbb7-48f0-974e-b735647a19ec"
     ]
    }
   ],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c75c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/kavyavenk/multilingual-backpacks.git\n",
    "%cd multilingual-backpacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c43c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets scipy tqdm numpy torch matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3249aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify GPU\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac1a744",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Europarl dataset (10k samples)\n",
    "!python data/europarl/prepare.py --language_pair en-fr --max_samples 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data files\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "data_files = ['data/europarl/train.bin', 'data/europarl/val.bin', 'data/europarl/meta.pkl']\n",
    "for f in data_files:\n",
    "    if os.path.exists(f):\n",
    "        size = os.path.getsize(f) / 1e6\n",
    "        print(f\"✓ {f} ({size:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"✗ {f} not found\")\n",
    "\n",
    "with open('data/europarl/meta.pkl', 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "    \n",
    "print(f\"\\nVocab size: {meta['vocab_size']:,}\")\n",
    "print(f\"Languages: {meta['languages']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa41b1e",
   "metadata": {},
   "source": [
    "## 3. Configure Transformer Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f665bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer baseline config from scratch\n",
    "config_content = \"\"\"\\\"\\\"\\\"\n",
    "Configuration for training Standard Transformer baseline on Europarl dataset\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "from configurator import ModelConfig\n",
    "\n",
    "config = ModelConfig(\n",
    "    # Model architecture\n",
    "    block_size=128,\n",
    "    n_layer=4,\n",
    "    n_head=4,\n",
    "    n_embd=256,\n",
    "    n_senses=1,  # Not used by transformer, but kept for compatibility\n",
    "    dropout=0.1,\n",
    "    bias=False,\n",
    "    \n",
    "    # Training\n",
    "    batch_size=16,\n",
    "    learning_rate=3e-4,\n",
    "    max_iters=2000,\n",
    "    weight_decay=1e-1,\n",
    "    beta1=0.9,\n",
    "    beta2=0.95,\n",
    "    grad_clip=1.0,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_interval=200,\n",
    "    eval_iters=50,\n",
    "    log_interval=10,\n",
    "    \n",
    "    # System\n",
    "    device='cuda',\n",
    "    dtype='float16',\n",
    "    compile=False,\n",
    "    \n",
    "    # Data\n",
    "    dataset='europarl',\n",
    "    tokenizer_name='xlm-roberta-base',\n",
    "    languages=['en', 'fr'],\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# Write config file\n",
    "with open('config/train_europarl_transformer_baseline.py', 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"✓ Transformer baseline config created\")\n",
    "print(\"\\nSettings:\")\n",
    "print(\"  Model: StandardTransformer (NO sense vectors)\")\n",
    "print(\"  Embedding dim: 256\")\n",
    "print(\"  Layers: 4\")\n",
    "print(\"  Heads: 4\")\n",
    "print(\"  Batch size: 16\")\n",
    "print(\"  Block size: 128\")\n",
    "print(\"  Max iterations: 2000\")\n",
    "print(\"\\nEstimated params: 250K vocab × 256 dim = 64M embeddings\")\n",
    "print(\"  + 4 layers × ~3M = ~76M total\")\n",
    "print(\"  Memory needed: ~6-8GB (should fit in T4!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844751f1",
   "metadata": {},
   "source": [
    "## 4. Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ee56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    mem_free = (mem_total - torch.cuda.memory_allocated(0) / 1e9)\n",
    "    \n",
    "    print(f\"GPU Memory:\")\n",
    "    print(f\"  Total: {mem_total:.2f} GB\")\n",
    "    print(f\"  Free: {mem_free:.2f} GB\")\n",
    "    print(f\"\\n✓ Ready for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0f57c",
   "metadata": {},
   "source": [
    "## 5. Train Transformer Baseline\n",
    "\n",
    "**Note**: Using `--model_type transformer` instead of `backpack`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train transformer baseline (NO sense vectors)\n",
    "!python train.py \\\n",
    "    --config train_europarl_transformer_baseline \\\n",
    "    --out_dir out/transformer_baseline \\\n",
    "    --data_dir europarl \\\n",
    "    --device cuda \\\n",
    "    --dtype float16 \\\n",
    "    --model_type transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec501528",
   "metadata": {},
   "source": [
    "## 6. Visualize Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13841deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('out/transformer_baseline/training_log.json', 'r') as f:\n",
    "    log = json.load(f)\n",
    "\n",
    "iterations = log['iterations']\n",
    "train_loss = log['train_loss']\n",
    "val_loss = log['val_loss']\n",
    "\n",
    "print(f\"Training Summary:\")\n",
    "print(f\"  Iterations: {len(iterations)}\")\n",
    "print(f\"  Initial train loss: {train_loss[0]:.4f}\")\n",
    "print(f\"  Final train loss: {train_loss[-1]:.4f}\")\n",
    "print(f\"  Final val loss: {val_loss[-1]:.4f}\")\n",
    "print(f\"  Loss reduction: {train_loss[0] - train_loss[-1]:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(iterations, train_loss, label='Train Loss', alpha=0.8, linewidth=2)\n",
    "plt.plot(iterations, val_loss, label='Val Loss', alpha=0.8, linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.title('Transformer Baseline Training (Europarl en-fr)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('out/transformer_baseline/loss_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Loss curves saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9764fe41",
   "metadata": {},
   "source": [
    "## 7. Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect model\n",
    "import torch\n",
    "from model import StandardTransformerLM\n",
    "\n",
    "checkpoint = torch.load('out/transformer_baseline/ckpt.pt', map_location='cuda')\n",
    "config = checkpoint['config']\n",
    "model = StandardTransformerLM(config)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRANSFORMER BASELINE MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Type: Standard Transformer (no sense vectors)\")\n",
    "print(f\"  Embedding dim: {config.n_embd}\")\n",
    "print(f\"  Layers: {config.n_layer}\")\n",
    "print(f\"  Attention heads: {config.n_head}\")\n",
    "print(f\"  Vocab size: {config.vocab_size:,}\")\n",
    "print(f\"  Context length: {config.block_size}\")\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Total: {n_params:,}\")\n",
    "print(f\"  Model size: {n_params * 4 / 1e6:.2f} MB (float32)\")\n",
    "print(f\"  Model size: {n_params * 2 / 1e6:.2f} MB (float16)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7d808",
   "metadata": {},
   "source": [
    "## 8. Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation suite\n",
    "!python run_full_evaluation.py \\\n",
    "    --out_dir out/transformer_baseline \\\n",
    "    --device cuda \\\n",
    "    --skip_multisimlex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52121271",
   "metadata": {},
   "source": [
    "## 9. Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d557161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package results\n",
    "!tar -czf transformer_baseline_results.tar.gz out/transformer_baseline/\n",
    "\n",
    "from google.colab import files \n",
    "files.download('transformer_baseline_results.tar.gz')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS PACKAGED\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nContents:\")\n",
    "print(\"  ✓ ckpt.pt - Model checkpoint\")\n",
    "print(\"  ✓ training_log.json - Training metrics\")\n",
    "print(\"  ✓ evaluation_results.json - Evaluation scores\")\n",
    "print(\"  ✓ loss_curves.png - Training visualization\")\n",
    "print(\"\\nThis baseline demonstrates cross-lingual learning\")\n",
    "print(\"without Backpack's sense vectors.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23935aab",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Trained\n",
    "- **Model**: Standard Transformer (no Backpack)\n",
    "- **Parameters**: ~76M (vs 60M for Backpack tiny)\n",
    "- **Why it works**: Only 1 embedding per token (not 4 sense vectors)\n",
    "- **Memory**: ~6-8GB (fits in T4's 15GB)\n",
    "\n",
    "### Key Findings\n",
    "1. Transformer baseline trains successfully on T4\n",
    "2. Can increase batch size for faster training\n",
    "3. Still learns cross-lingual representations from Europarl\n",
    "\n",
    "### Next Steps\n",
    "- Compare with Backpack model (when vocab size is reduced)\n",
    "- Evaluate cross-lingual word/sentence similarity\n",
    "- Document differences in learned representations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
