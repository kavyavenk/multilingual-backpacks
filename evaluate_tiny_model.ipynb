{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete Evaluation Suite for Tiny Backpack Model\n",
        "\n",
        "This notebook runs all evaluation methods on the trained tiny model:\n",
        "- Word-level representations\n",
        "- Cross-lingual word similarity\n",
        "- Sense vector analysis (Backpack only)\n",
        "- Sentence-level representations\n",
        "- Cross-lingual sentence similarity\n",
        "- MultiSimLex benchmark evaluation\n",
        "\n",
        "**Note**: Make sure you have a trained model checkpoint in `out/tiny/ckpt.pt`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All imports successful\n"
          ]
        }
      ],
      "source": [
        "# Setup and imports\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('.')\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "from transformers import AutoTokenizer\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Import our models and utilities\n",
        "from model import BackpackLM, StandardTransformerLM\n",
        "from configurator import get_config\n",
        "from evaluate import (\n",
        "    load_model,\n",
        "    get_word_representations,\n",
        "    get_sentence_representation,\n",
        "    evaluate_word_similarity,\n",
        "    evaluate_sentence_similarity,\n",
        "    analyze_sense_vectors,\n",
        "    evaluate_multisimlex,\n",
        "    evaluate_cross_lingual_multisimlex\n",
        ")\n",
        "\n",
        "print(\"✓ All imports successful\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU Setup\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"⚠️  CUDA not available, using CPU (will be slower)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "out_dir = 'out/tiny'\n",
        "tokenizer_name = 'xlm-roberta-base'\n",
        "\n",
        "print(f\"Loading model from {out_dir}...\")\n",
        "model, config = load_model(out_dir, device)\n",
        "\n",
        "print(f\"\\nModel Configuration:\")\n",
        "print(f\"  Type: {'Backpack' if isinstance(model, BackpackLM) else 'Standard Transformer'}\")\n",
        "print(f\"  Vocab size: {config.vocab_size}\")\n",
        "print(f\"  Embedding dim: {config.n_embd}\")\n",
        "if isinstance(model, BackpackLM):\n",
        "    print(f\"  Number of senses: {model.n_senses}\")\n",
        "print(f\"  Block size: {config.block_size}\")\n",
        "print(f\"  Layers: {config.n_layer}\")\n",
        "\n",
        "print(f\"\\nLoading tokenizer: {tokenizer_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "print(f\"✓ Tokenizer loaded (vocab size: {tokenizer.vocab_size})\")\n",
        "\n",
        "# Count parameters\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nTotal parameters: {n_params:,} ({n_params/1e6:.2f}M)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Word-Level Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract word representations\n",
        "test_words_en = ['hello', 'world', 'language', 'model', 'learning', 'bank', 'star']\n",
        "test_words_fr = ['bonjour', 'monde', 'langue', 'modèle', 'apprentissage', 'banque', 'étoile']\n",
        "\n",
        "print(\"\\n=== English Word Representations ===\")\n",
        "en_reprs = get_word_representations(model, tokenizer, test_words_en, device)\n",
        "for word, repr in en_reprs.items():\n",
        "    print(f\"  {word:15s}: shape {repr.shape}\")\n",
        "\n",
        "print(\"\\n=== French Word Representations ===\")\n",
        "fr_reprs = get_word_representations(model, tokenizer, test_words_fr, device)\n",
        "for word, repr in fr_reprs.items():\n",
        "    print(f\"  {word:15s}: shape {repr.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-lingual word similarity\n",
        "translation_pairs = [\n",
        "    ('hello', 'bonjour'),\n",
        "    ('world', 'monde'),\n",
        "    ('language', 'langue'),\n",
        "    ('model', 'modèle'),\n",
        "    ('learning', 'apprentissage'),\n",
        "    ('bank', 'banque'),\n",
        "    ('star', 'étoile'),\n",
        "]\n",
        "\n",
        "print(\"\\n=== Cross-lingual Word Similarity ===\")\n",
        "similarities = evaluate_word_similarity(model, tokenizer, translation_pairs, device)\n",
        "\n",
        "print(\"\\nTranslation pair similarities:\")\n",
        "for word1, word2, sim in similarities:\n",
        "    # Color code based on similarity\n",
        "    if sim > 0.7:\n",
        "        status = \"✓ Excellent\"\n",
        "    elif sim > 0.5:\n",
        "        status = \"○ Good\"\n",
        "    else:\n",
        "        status = \"⚠ Needs improvement\"\n",
        "    print(f\"  {word1:15s} <-> {word2:15s}: {sim:6.4f}  {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Sense Vector Analysis (Backpack Only)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze sense vectors (only for Backpack models)\n",
        "if isinstance(model, BackpackLM):\n",
        "    print(\"\\n=== Sense Vector Analysis ===\")\n",
        "    \n",
        "    # Analyze words with multiple meanings\n",
        "    polysemous_words = ['bank', 'star', 'model']\n",
        "    \n",
        "    for word in polysemous_words:\n",
        "        print(f\"\\n{word.upper()}:\")\n",
        "        senses = analyze_sense_vectors(model, tokenizer, [word], device, top_k=5)\n",
        "        if word in senses:\n",
        "            for sense_idx, predictions in enumerate(senses[word]):\n",
        "                print(f\"  Sense {sense_idx}: {predictions}\")\n",
        "else:\n",
        "    print(\"\\n⚠️  Sense vector analysis only available for Backpack models\")\n",
        "    print(\"   Current model is Standard Transformer (no sense vectors)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sentence-Level Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentence representations\n",
        "test_sentences_en = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"The language model is learning.\",\n",
        "    \"This is a test sentence.\",\n",
        "    \"Machine learning is fascinating.\",\n",
        "    \"Natural language processing helps computers understand text.\",\n",
        "]\n",
        "\n",
        "test_sentences_fr = [\n",
        "    \"Bonjour, comment allez-vous?\",\n",
        "    \"Le modèle de langue apprend.\",\n",
        "    \"Ceci est une phrase de test.\",\n",
        "    \"L'apprentissage automatique est fascinant.\",\n",
        "    \"Le traitement du langage naturel aide les ordinateurs à comprendre le texte.\",\n",
        "]\n",
        "\n",
        "print(\"\\n=== Sentence Representations ===\")\n",
        "print(\"\\nEnglish sentences:\")\n",
        "for sent in test_sentences_en:\n",
        "    repr = get_sentence_representation(model, tokenizer, sent, device, method='mean')\n",
        "    print(f\"  {sent[:50]:50s}: shape {repr.shape}\")\n",
        "\n",
        "print(\"\\nFrench sentences:\")\n",
        "for sent in test_sentences_fr:\n",
        "    repr = get_sentence_representation(model, tokenizer, sent, device, method='mean')\n",
        "    print(f\"  {sent[:50]:50s}: shape {repr.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-lingual sentence similarity\n",
        "sentence_pairs = list(zip(test_sentences_en, test_sentences_fr))\n",
        "\n",
        "print(\"\\n=== Cross-lingual Sentence Similarity ===\")\n",
        "sent_similarities = evaluate_sentence_similarity(model, tokenizer, sentence_pairs, device)\n",
        "\n",
        "print(\"\\nTranslation pair sentence similarities:\")\n",
        "for sent1, sent2, sim in sent_similarities:\n",
        "    if sim > 0.8:\n",
        "        status = \"✓ Excellent\"\n",
        "    elif sim > 0.6:\n",
        "        status = \"○ Good\"\n",
        "    else:\n",
        "        status = \"⚠ Needs improvement\"\n",
        "    print(f\"\\n  EN: {sent1[:60]}...\")\n",
        "    print(f\"  FR: {sent2[:60]}...\")\n",
        "    print(f\"  Similarity: {sim:.4f}  {status}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. MultiSimLex Benchmark Evaluation\n",
        "\n",
        "**Configure subset size** (for faster evaluation):\n",
        "- Set `max_samples` to limit the number of word pairs evaluated\n",
        "- Example: `max_samples=100` evaluates only first 100 pairs (faster)\n",
        "- Set `max_samples=None` to evaluate all pairs (slower but more accurate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration: Subset size for MultiSimLex evaluation\n",
        "# Set to None to evaluate all pairs, or a number (e.g., 100) for faster evaluation\n",
        "max_samples = 100  # Change this: None = all pairs, 100 = first 100 pairs, etc.\n",
        "\n",
        "print(f\"MultiSimLex evaluation will use: {max_samples if max_samples else 'ALL'} word pairs\")\n",
        "\n",
        "# Check if datasets library is available\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    datasets_available = True\n",
        "except ImportError:\n",
        "    print(\"⚠️  datasets library not installed. Install with: pip install datasets\")\n",
        "    datasets_available = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MultiSimLex monolingual evaluation\n",
        "if datasets_available:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"MultiSimLex Benchmark Evaluation\")\n",
        "    if max_samples:\n",
        "        print(f\"Using subset: {max_samples} word pairs per language\")\n",
        "    else:\n",
        "        print(\"Using full dataset\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # English evaluation\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    result_en = evaluate_multisimlex(model, tokenizer, device, language='en', max_samples=max_samples)\n",
        "    if result_en:\n",
        "        results['en'] = result_en\n",
        "    \n",
        "    # French evaluation\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    result_fr = evaluate_multisimlex(model, tokenizer, device, language='fr', max_samples=max_samples)\n",
        "    if result_fr:\n",
        "        results['fr'] = result_fr\n",
        "    \n",
        "    # Cross-lingual evaluation\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    result_cross = evaluate_cross_lingual_multisimlex(model, tokenizer, device, 'en', 'fr', max_samples=max_samples)\n",
        "    if result_cross:\n",
        "        results['en-fr'] = result_cross\n",
        "    \n",
        "    # Summary\n",
        "    if results:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"MultiSimLex Summary\")\n",
        "        print(\"=\"*60)\n",
        "        for key, result in results.items():\n",
        "            print(f\"{key.upper():10s}: {result['correlation']:.4f} ({result['benchmark_level']}) - {result['n_pairs']} pairs\")\n",
        "else:\n",
        "    print(\"\\n⚠️  Skipping MultiSimLex evaluation (datasets library not available)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Loss Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display training log\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "log_file = os.path.join(out_dir, 'training_log.json')\n",
        "\n",
        "if os.path.exists(log_file):\n",
        "    with open(log_file, 'r') as f:\n",
        "        training_log = json.load(f)\n",
        "    \n",
        "    iterations = training_log['iterations']\n",
        "    train_loss = training_log['train_loss']\n",
        "    val_loss = training_log['val_loss']\n",
        "    \n",
        "    if len(iterations) > 0:\n",
        "        print(f\"\\n=== Training Progress ===\")\n",
        "        print(f\"Total evaluations: {len(iterations)}\")\n",
        "        print(f\"Latest iteration: {iterations[-1]}\")\n",
        "        print(f\"Latest train loss: {train_loss[-1]:.4f}\")\n",
        "        print(f\"Latest val loss: {val_loss[-1]:.4f}\")\n",
        "        \n",
        "        # Plot loss curves\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        \n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(iterations, train_loss, label='Train Loss', linewidth=2, alpha=0.8)\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(iterations, val_loss, label='Val Loss', linewidth=2, alpha=0.8, color='orange')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Validation Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Top activating words (if available)\n",
        "        if 'top_activating_words' in training_log and len(training_log['top_activating_words']) > 0:\n",
        "            print(f\"\\n=== Top Activating Words (from training log) ===\")\n",
        "            latest_words = training_log['top_activating_words'][-1]\n",
        "            print(f\"Iteration {latest_words['iteration']}:\")\n",
        "            for word_info in latest_words['words'][:10]:\n",
        "                print(f\"  {word_info['word']:20s}: activation {word_info['activation']:.4f}\")\n",
        "    else:\n",
        "        print(\"\\n⚠️  No training data in log file yet\")\n",
        "else:\n",
        "    print(f\"\\n⚠️  Training log not found: {log_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate summary report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nModel: {'Backpack LM' if isinstance(model, BackpackLM) else 'Standard Transformer'}\")\n",
        "print(f\"Parameters: {n_params:,} ({n_params/1e6:.2f}M)\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "print(f\"\\n✓ Word representations extracted\")\n",
        "print(f\"✓ Cross-lingual word similarity computed\")\n",
        "if isinstance(model, BackpackLM):\n",
        "    print(f\"✓ Sense vector analysis completed\")\n",
        "print(f\"✓ Sentence representations extracted\")\n",
        "print(f\"✓ Cross-lingual sentence similarity computed\")\n",
        "\n",
        "if datasets_available and 'results' in locals():\n",
        "    print(f\"✓ MultiSimLex benchmark evaluation completed\")\n",
        "    if results:\n",
        "        print(f\"\\nMultiSimLex Results:\")\n",
        "        for key, result in results.items():\n",
        "            print(f\"  {key.upper()}: {result['correlation']:.4f} ({result['benchmark_level']})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"All evaluations complete!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
