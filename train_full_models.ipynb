{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full-Sized Model Training (Memory-Optimized)\n",
        "\n",
        "This notebook trains both Backpack and Transformer baseline models on Europarl English-French data with 16 senses.\n",
        "\n",
        "**Models:**\n",
        "- Backpack: ~1.02B parameters (16 senses, 4 layers, 4 heads, 256 embd)\n",
        "- Transformer: ~64M parameters (same architecture, no senses)\n",
        "\n",
        "**Configuration:**\n",
        "- Uses parameters from `train_backpack_clean` but with 16 senses\n",
        "- Memory-optimized: smaller batch size (8), no compilation, smaller context (128)\n",
        "- Should fit on most GPUs without OOM errors\n",
        "\n",
        "**Training:**\n",
        "- 50,000 iterations max\n",
        "- Automatic checkpoint saving (best val loss + periodic every 500 iterations)\n",
        "- Resume capability if interrupted\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone and setup\n",
        "!git clone https://github.com/kavyavenk/multilingual-backpacks.git\n",
        "%cd multilingual-backpacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount Drive for checkpoints (optional)\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    drive_checkpoint_dir = '/content/drive/MyDrive/multilingual-backpacks-checkpoints'\n",
        "    os.makedirs(drive_checkpoint_dir, exist_ok=True)\n",
        "    USE_DRIVE = True\n",
        "    print(\"✓ Drive mounted - checkpoints will be saved\")\n",
        "except Exception as e:\n",
        "    USE_DRIVE = False\n",
        "    print(\"⚠️  Drive not mounted - checkpoints will only be local\")\n",
        "    print(\"   (You can download results at the end)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers datasets scipy tqdm matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPU\n",
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare Europarl dataset (full dataset - remove max_samples limit for full training)\n",
        "!python data/europarl/prepare.py --language_pair en-fr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "with open('data/europarl/meta.pkl', 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "\n",
        "print(f\"Vocab size: {meta['vocab_size']:,}\")\n",
        "print(f\"Languages: {meta['languages']}\")\n",
        "\n",
        "for fname in ['train.bin', 'val.bin']:\n",
        "    fpath = f\"data/europarl/{fname}\"\n",
        "    if os.path.exists(fpath):\n",
        "        size = os.path.getsize(fpath) / 1e6\n",
        "        print(f\"✓ {fname}: {size:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Restore Previous Checkpoints (if any)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restore previous checkpoints if exist\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "for model_type in ['backpack', 'transformer']:\n",
        "    if USE_DRIVE:\n",
        "        drive_dir = f'/content/drive/MyDrive/multilingual-backpacks-checkpoints/{model_type}_full'\n",
        "        local_dir = f'out/{model_type}_full'\n",
        "        \n",
        "        if os.path.exists(drive_dir):\n",
        "            shutil.copytree(drive_dir, local_dir, dirs_exist_ok=True)\n",
        "            print(f\"✓ Restored {model_type} checkpoint from Drive\")\n",
        "            \n",
        "            if os.path.exists(f\"{local_dir}/training_log.json\"):\n",
        "                with open(f\"{local_dir}/training_log.json\") as f:\n",
        "                    log = json.load(f)\n",
        "                print(f\"  Completed {len(log['iterations'])} iterations\")\n",
        "                if log['iterations']:\n",
        "                    print(f\"  Last train loss: {log['train_loss'][-1]:.4f}\")\n",
        "                    print(f\"  Last val loss: {log['val_loss'][-1]:.4f}\")\n",
        "        else:\n",
        "            print(f\"No previous {model_type} checkpoint found - starting fresh\")\n",
        "    else:\n",
        "        print(f\"Drive not available - {model_type} will start fresh\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train Backpack Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine if we should resume or start fresh\n",
        "import os\n",
        "backpack_ckpt = 'out/backpack_full/ckpt.pt'\n",
        "init_from = 'resume' if os.path.exists(backpack_ckpt) else 'scratch'\n",
        "print(f\"Backpack: {init_from}\")\n",
        "\n",
        "# Train Backpack model (using memory-optimized config with 16 senses)\n",
        "!python train.py \\\n",
        "    --model_type backpack \\\n",
        "    --config train_europarl_scratch_16senses \\\n",
        "    --out_dir out/backpack_full \\\n",
        "    --data_dir europarl \\\n",
        "    --init_from {init_from} \\\n",
        "    --device cuda \\\n",
        "    --dtype float16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final backup for Backpack (if Drive available)\n",
        "if USE_DRIVE:\n",
        "    drive_dir = '/content/drive/MyDrive/multilingual-backpacks-checkpoints/backpack_full'\n",
        "    shutil.copytree('out/backpack_full', drive_dir, dirs_exist_ok=True)\n",
        "    print(\"✓ Backpack checkpoint saved to Drive\")\n",
        "else:\n",
        "    print(\"✓ Backpack training complete - checkpoint saved locally\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Transformer Baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine if we should resume or start fresh\n",
        "transformer_ckpt = 'out/transformer_full/ckpt.pt'\n",
        "init_from = 'resume' if os.path.exists(transformer_ckpt) else 'scratch'\n",
        "print(f\"Transformer: {init_from}\")\n",
        "\n",
        "# Train Transformer baseline model (matching backpack config)\n",
        "!python train.py \\\n",
        "    --model_type transformer \\\n",
        "    --config train_europarl_transformer_baseline_16senses \\\n",
        "    --out_dir out/transformer_full \\\n",
        "    --data_dir europarl \\\n",
        "    --init_from {init_from} \\\n",
        "    --device cuda \\\n",
        "    --dtype float16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final backup for Transformer (if Drive available)\n",
        "if USE_DRIVE:\n",
        "    drive_dir = '/content/drive/MyDrive/multilingual-backpacks-checkpoints/transformer_full'\n",
        "    shutil.copytree('out/transformer_full', drive_dir, dirs_exist_ok=True)\n",
        "    print(\"✓ Transformer checkpoint saved to Drive\")\n",
        "else:\n",
        "    print(\"✓ Transformer training complete - checkpoint saved locally\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualize Training Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot loss curves for both models\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "for idx, (model_type, model_name) in enumerate([('backpack', 'Backpack'), ('transformer', 'Transformer')]):\n",
        "    log_path = f'out/{model_type}_full/training_log.json'\n",
        "    if os.path.exists(log_path):\n",
        "        with open(log_path) as f:\n",
        "            log = json.load(f)\n",
        "        \n",
        "        ax = axes[idx]\n",
        "        ax.plot(log['iterations'], log['train_loss'], label='Train', linewidth=2)\n",
        "        ax.plot(log['iterations'], log['val_loss'], label='Val', linewidth=2)\n",
        "        ax.set_xlabel('Iteration')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.set_title(f'{model_name} Training on Europarl en-fr')\n",
        "        ax.legend()\n",
        "        ax.grid(alpha=0.3)\n",
        "        \n",
        "        print(f\"\\n{model_name} Summary:\")\n",
        "        print(f\"  Iterations: {len(log['iterations'])}\")\n",
        "        if log['iterations']:\n",
        "            print(f\"  Final train loss: {log['train_loss'][-1]:.4f}\")\n",
        "            print(f\"  Final val loss: {log['val_loss'][-1]:.4f}\")\n",
        "            print(f\"  Loss reduction: {log['train_loss'][0] - log['train_loss'][-1]:.4f}\")\n",
        "    else:\n",
        "        axes[idx].text(0.5, 0.5, f'No training log found\\nfor {model_name}', \n",
        "                      ha='center', va='center', transform=axes[idx].transAxes)\n",
        "        axes[idx].set_title(f'{model_name} Training')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('out/training_comparison.png', dpi=150)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display model information\n",
        "import torch\n",
        "from configurator import get_config\n",
        "\n",
        "for model_type, model_name in [('backpack', 'Backpack'), ('transformer', 'Transformer')]:\n",
        "    ckpt_path = f'out/{model_type}_full/ckpt.pt'\n",
        "    if os.path.exists(ckpt_path):\n",
        "        checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
        "        config = checkpoint['config']\n",
        "        \n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"{model_name.upper()} MODEL\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Embedding dim: {config.n_embd}\")\n",
        "        if model_type == 'backpack':\n",
        "            print(f\"Sense vectors: {config.n_senses}\")\n",
        "        print(f\"Layers: {config.n_layer}\")\n",
        "        print(f\"Heads: {config.n_head}\")\n",
        "        print(f\"Vocab: {config.vocab_size:,}\")\n",
        "        print(f\"Block size: {config.block_size}\")\n",
        "        print(f\"\\nTraining:\")\n",
        "        print(f\"  Iterations: {checkpoint.get('iter_num', 0):,}\")\n",
        "        print(f\"  Best val loss: {checkpoint.get('best_val_loss', 'N/A'):.4f}\")\n",
        "        \n",
        "        # Calculate model size\n",
        "        model_state = checkpoint['model']\n",
        "        total_params = sum(p.numel() for p in model_state.values())\n",
        "        model_size_mb = total_params * 2 / 1e6  # float16 = 2 bytes\n",
        "        \n",
        "        print(f\"\\nTotal params: {total_params:,}\")\n",
        "        print(f\"Size: {model_size_mb:.1f} MB (float16)\")\n",
        "        print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Download Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Package all results\n",
        "import tarfile\n",
        "\n",
        "with tarfile.open('full_models_results.tar.gz', 'w:gz') as tar:\n",
        "    for model_type in ['backpack_full', 'transformer_full']:\n",
        "        model_dir = f'out/{model_type}'\n",
        "        if os.path.exists(model_dir):\n",
        "            tar.add(model_dir, arcname=model_type)\n",
        "            print(f\"✓ Added {model_type}\")\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "files.download('full_models_results.tar.gz')\n",
        "\n",
        "print(\"\\n✓ Results downloaded\")\n",
        "print(\"\\nContents:\")\n",
        "print(\"  - backpack_full/ckpt.pt: Backpack model checkpoint\")\n",
        "print(\"  - backpack_full/training_log.json: Backpack training metrics\")\n",
        "print(\"  - transformer_full/ckpt.pt: Transformer model checkpoint\")\n",
        "print(\"  - transformer_full/training_log.json: Transformer training metrics\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
